---
title: ï¿½»é›¶å¼€å§‹å­¦ä¹ LoRAï¼šMiniMindé¡¹ç›®å®æˆ˜æŒ‡å—
date: 2026-02-22
timestamp: 2026-02-22T16:29:46+08:00
slug: loraminimind
category: note
tags:
  - AI
  - LLM
---

# ä»é›¶å¼€å§‹å­¦ä¹ LoRAï¼šMiniMindé¡¹ç›®å®æˆ˜æŒ‡å—

## ç†è®ºåŸºç¡€

### ä»€ä¹ˆæ˜¯LoRAï¼Ÿ

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œç”±å¾®è½¯åœ¨2021å¹´æå‡ºã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**å†»ç»“é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œåªè®­ç»ƒå°‘é‡ä½ç§©çŸ©é˜µæ¥é€‚é…æ–°ä»»åŠ¡**ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ è¦è®©ä¸€ä¸ªä¼šå¼¹é’¢ç´çš„äººå»å­¦æ‹‰å°æç´ã€‚ä¼ ç»Ÿçš„åšæ³•æ˜¯é‡æ–°æ•™ä»–æ‰€æœ‰æŠ€å·§ï¼ˆå…¨å‚æ•°å¾®è°ƒï¼‰ï¼Œè€ŒLoRAçš„åšæ³•æ˜¯ï¼šä¿æŒä»–åŸæœ‰çš„é’¢ç´æŠ€è‰ºä¸å˜ï¼Œåªæ•™ä»–å‡ ä¸ªç‰¹æ®Šçš„æŒ‡æ³•æŠ€å·§ï¼ˆä½ç§©é€‚é…ï¼‰ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹©LoRAï¼Ÿ

1. **å‚æ•°é‡æå°‘**ï¼šå¯¹äºä¸€ä¸ª40Bæ¨¡å‹ï¼Œrank=8æ—¶LoRAå‚æ•°åªå 0.1-1%
2. **è®­ç»ƒé€Ÿåº¦å¿«**ï¼šåªéœ€ä¼˜åŒ–å°‘é‡ä½ç§©å‚æ•°
3. **å†…å­˜æ•ˆç‡é«˜**ï¼šæ— éœ€å­˜å‚¨å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬
4. **å¯ç»„åˆæ€§å¼º**ï¼šå¯ä»¥åœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸ŠåŠ è½½å¤šä¸ªLoRAé€‚é…å™¨

## æ ¸å¿ƒæ¶æ„

### LoRAç½‘ç»œç»“æ„

è®©æˆ‘å…ˆä»æœ€æ ¸å¿ƒçš„LoRAç±»å¼€å§‹åˆ†æï¼š

```python
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank):
        self.rank = rank  # ä½ç§©ç§©å€¼ï¼ˆé€šå¸¸å¾ˆå°ï¼Œå¦‚4-16ï¼‰
        self.A = nn.Linear(in_features, rank, bias=False)   # é™ç§©çŸ©é˜µ rÃ—d
        self.B = nn.Linear(rank, out_features, bias=False)  # å‡ç§©çŸ©é˜µ d'Ã—r
```

**å…³é”®è®¾è®¡ç‚¹ï¼š**

- **ä½ç§©åˆ†è§£**ï¼šåŸå§‹æƒé‡çŸ©é˜µ Wï¼ˆdÃ—d'ï¼‰è¢«åˆ†è§£ä¸º W + BAï¼Œå…¶ä¸­ BA çš„ç§©ä¸º rï¼ˆr << dï¼‰
  - æ•°å­¦åŸç†ï¼šä»»ä½•çŸ©é˜µéƒ½å¯ä»¥åˆ†è§£ä¸ºä½ç§©çŸ©é˜µçš„ä¹˜ç§¯
  - å®é™…æ•ˆæœï¼šç”¨å°‘é‡çš„å‚æ•°å°±èƒ½è¡¨ç¤ºåŸå§‹æƒé‡çš„"å°å¹…åº¦è°ƒæ•´"

- **åˆå§‹åŒ–ç­–ç•¥**ï¼šçŸ©é˜µAä½¿ç”¨é«˜æ–¯åˆå§‹åŒ–ï¼ˆstd=0.02ï¼‰ï¼ŒçŸ©é˜µBåˆå§‹åŒ–ä¸ºé›¶
  - AçŸ©é˜µï¼šé«˜æ–¯åˆ†å¸ƒæä¾›è‰¯å¥½çš„æ¢¯åº¦æµåŠ¨
  - BçŸ©é˜µï¼šé›¶åˆå§‹åŒ–ç¡®ä¿è®­ç»ƒåˆæœŸ=Wï¼Œé¿å…çªç„¶æ”¹å˜

- **æ— åç½®**ï¼šä¸¤ä¸ªçŸ©é˜µéƒ½ä½¿ç”¨bias=Falseï¼Œé¿å…å‚æ•°å†—ä½™

**ä¸ºä»€ä¹ˆé€‰æ‹©ä½ç§©ï¼Ÿ**
æƒ³è±¡ä¸€ä¸‹ä½ è¦è°ƒæ•´ä¸€å¼ ç…§ç‰‡çš„äº®åº¦ï¼š
- å…¨å‚æ•°å¾®è°ƒï¼šé‡æ–°è°ƒæ•´æ¯ä¸ªåƒç´ ï¼ˆå¤ªæ˜‚è´µï¼‰
- LoRAï¼šåªè°ƒæ•´ä¸€ä¸ª"äº®åº¦æ»‘å—"ï¼ˆæˆæœ¬ä½ï¼Œæ•ˆæœå¥½ï¼‰

rankå€¼é€‰æ‹©å‚è€ƒï¼š
- rank=4-8ï¼šå°æ¨¡å‹ï¼Œé€‚åˆç®€å•ä»»åŠ¡
- rank=8-16ï¼šä¸­ç­‰æ¨¡å‹ï¼Œé€šç”¨æ€§å¼º
- rank>16ï¼šå¤§æ¨¡å‹ï¼Œæ¥è¿‘å…¨å¾®è°ƒæ•ˆæœ

### Module nameçš„æ¥æº

è¿™æ˜¯æˆ‘å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°çš„ä¸€ä¸ªéš¾ç‚¹ï¼šå½“ä½ è°ƒç”¨`model.named_modules()`æ—¶ï¼ŒPyTorchä¼šé€’å½’åœ°éå†æ¨¡å‹ä¸­å®šä¹‰çš„æ‰€æœ‰å­æ¨¡å—ï¼Œå¹¶æ ¹æ®ä½ åœ¨`__init__`ä¸­ç»™å®ƒä»¬èµ·çš„å˜é‡åç”Ÿæˆä¸€æ¡è·¯å¾„ã€‚

ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼š
```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.transformer = TransformerBlock() # å˜é‡åæ˜¯ transformer

class TransformerBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.q_proj = nn.Linear(512, 512) # å˜é‡åæ˜¯ q_proj
```

å½“ä½ è¿è¡Œ`apply_lora(model)`åï¼Œ`q_proj`å†…éƒ¨è¢«æŒ‚è½½äº†ä¸€ä¸ªåä¸º`lora`çš„å­æ¨¡å—ã€‚æ­¤æ—¶ï¼Œ`named_modules()`è¿”å›çš„nameå°±ä¼šæ˜¯ï¼š

```
transformer
transformer.q_proj
transformer.q_proj.loraï¼ˆè¿™å°±æ˜¯ä½ è¦æ‰¾çš„åå­—ï¼ï¼‰
```


### å‚æ•°åŒ–æ”¹é€ 

```python
def apply_lora(model, rank=8):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
            # åªä¸ºæ–¹å½¢æƒé‡çŸ©é˜µæ·»åŠ LoRA
            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank)
            setattr(module, "lora", lora)
            # ä¿®æ”¹forwardå‡½æ•°
            original_forward = module.forward
            def new_forward(x, original_forward=original_forward, lora=lora):
                return original_forward(x) + lora(x)
            module.forward = new_forward
```

**æ”¹é€ ç­–ç•¥ï¼š**

- åªå¤„ç†æ–¹å½¢çŸ©é˜µï¼ˆtransformerä¸­çš„æ³¨æ„åŠ›å±‚é€šå¸¸æ˜¯æ–¹é˜µï¼‰
- åœ¨æ¯ä¸ªçº¿æ€§å±‚å†…éƒ¨æ·»åŠ LoRAæ¨¡å—
- é€šè¿‡monkey patchingä¿®æ”¹forwardå‡½æ•°ï¼š`output = original_forward(x) + lora(x)`

## è®­ç»ƒæµç¨‹

### è®­ç»ƒé…ç½®

```python
# å…³é”®è¶…å‚æ•°
args.learning_rate = 1e-4      # è¾ƒä½çš„å­¦ä¹ ç‡ï¼Œé¿å…ç ´åé¢„è®­ç»ƒæƒé‡
args.epochs = 50               # è®­ç»ƒè½®æ•°
args.batch_size = 32           # æ‰¹æ¬¡å¤§å°
args.rank = 8                  # LoRAç§©å€¼
```

### å‚æ•°å†»ç»“ç­–ç•¥

```python
# å†»ç»“éLoRAå‚æ•°ï¼Œåªè®­ç»ƒLoRAå‚æ•°
for name, param in model.named_parameters():
    if 'lora' in name:
        param.requires_grad = True
        lora_params.append(param)
    else:
        param.requires_grad = False
```

**ä¼˜åŠ¿ï¼š**

- **å‚æ•°é‡æå°‘**ï¼šå¯¹äºä¸€ä¸ª40Bæ¨¡å‹ï¼Œrank=8æ—¶LoRAå‚æ•°åªå 0.1-1%
  - ä¸¾ä¾‹ï¼šBERT-large (355Må‚æ•°)ï¼ŒLoRA rank=8æ—¶åªéœ€çº¦1.4Må‚æ•°
  - å­˜å‚¨ç©ºé—´ï¼šä¸€ä¸ª40Bæ¨¡å‹ï¼ŒLoRAæƒé‡å¯èƒ½åªæœ‰å‡ ç™¾MB

- **è®­ç»ƒé€Ÿåº¦å¿«**ï¼šåªéœ€ä¼˜åŒ–å°‘é‡ä½ç§©å‚æ•°
  - è®­ç»ƒæ—¶é—´ï¼šé€šå¸¸æ¯”å…¨å¾®è°ƒå¿«5-10å€
  - æ”¶æ•›é€Ÿåº¦ï¼šLoRAå‚æ•°å°‘ï¼Œæ›´å®¹æ˜“æ”¶æ•›

- **å†…å­˜æ•ˆç‡é«˜**ï¼šæ— éœ€å­˜å‚¨å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬
  - æ¨ç†æ—¶ï¼šä¸€ä¸ªæ¨¡å‹ + å¤šä¸ªå°LoRAæ–‡ä»¶
  - éƒ¨ç½²æ—¶ï¼šå¯ä»¥åŠ¨æ€åˆ‡æ¢ä¸åŒçš„LoRAé€‚é…å™¨

### æƒé‡ç®¡ç†

#### LoRAæƒé‡ä¿å­˜

```python
def save_lora(model, path):
    state_dict = {}
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()}
            state_dict.update(lora_state)
    torch.save(state_dict, path)
```

#### LoRAæƒé‡åŠ è½½

```python
def load_lora(model, path):
    state_dict = torch.load(path, map_location=model.device)
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items()
                         if f'{name}.lora.' in k}
            module.lora.load_state_dict(lora_state)
```

#### æ–‡ä»¶ç»“æ„ï¼š

ä¸»æ¨¡å‹ï¼š`./checkpoints/model_name_hidden_size.pth`
LoRAæƒé‡ï¼š`./out/lora/lora_name_hidden_size.pth`

## æ¨ç†éƒ¨ç½²

### åŠ¨æ€åŠ è½½LoRA

```python
if args.lora_weight != 'None':
    apply_lora(model)              # åº”ç”¨LoRAç»“æ„
    load_lora(model, lora_path)    # åŠ è½½LoRAæƒé‡
```

### å®æ—¶åˆ‡æ¢èƒ½åŠ›

å¯ä»¥åœ¨åŒä¸€ä¸ªä¸»æ¨¡å‹ä¸ŠåŠ è½½ä¸åŒçš„LoRAæƒé‡
- æ”¯æŒé›¶æ ·æœ¬æç¤ºï¼ˆå°‘æ ·æœ¬å­¦ä¹ ï¼‰
- æ¯ä¸ªLoRAæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªç‰¹å®šä»»åŠ¡çš„é€‚é…

**å®é™…åº”ç”¨åœºæ™¯ï¼š**
æ¯”å¦‚ä½ å¯ä»¥æœ‰ä¸€ä¸ªåŸºç¡€çš„MiniMindæ¨¡å‹ï¼Œç„¶åé’ˆå¯¹ä¸åŒçš„ä»»åŠ¡è®­ç»ƒä¸åŒçš„LoRAé€‚é…å™¨ï¼š
- `lora_translation_768.pth` - ä¸­æ–‡ç¿»è¯‘ä»»åŠ¡
- `lora_qa_768.pth` - é—®ç­”ä»»åŠ¡
- `lora_summary_768.pth` - æ–‡æœ¬æ€»ç»“ä»»åŠ¡

æ¨ç†æ—¶åªéœ€è¦åŠ è½½å¯¹åº”çš„LoRAæ–‡ä»¶å³å¯ï¼Œæ— éœ€é‡æ–°åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼Œè¿™å¤§å¤§èŠ‚çœäº†å†…å­˜å’Œæ¨ç†æ—¶é—´ã€‚